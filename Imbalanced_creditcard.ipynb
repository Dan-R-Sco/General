{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas  as pd \nimport matplotlib.pyplot as plt \nimport numpy as np \nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.metrics import confusion_matrix, classification_report \n  \n# load the data set \ndata = pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv') \n  \n# print info about columns in the dataframe \nprint(data.info()) ","execution_count":10,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 284807 entries, 0 to 284806\nData columns (total 31 columns):\n #   Column  Non-Null Count   Dtype  \n---  ------  --------------   -----  \n 0   Time    284807 non-null  float64\n 1   V1      284807 non-null  float64\n 2   V2      284807 non-null  float64\n 3   V3      284807 non-null  float64\n 4   V4      284807 non-null  float64\n 5   V5      284807 non-null  float64\n 6   V6      284807 non-null  float64\n 7   V7      284807 non-null  float64\n 8   V8      284807 non-null  float64\n 9   V9      284807 non-null  float64\n 10  V10     284807 non-null  float64\n 11  V11     284807 non-null  float64\n 12  V12     284807 non-null  float64\n 13  V13     284807 non-null  float64\n 14  V14     284807 non-null  float64\n 15  V15     284807 non-null  float64\n 16  V16     284807 non-null  float64\n 17  V17     284807 non-null  float64\n 18  V18     284807 non-null  float64\n 19  V19     284807 non-null  float64\n 20  V20     284807 non-null  float64\n 21  V21     284807 non-null  float64\n 22  V22     284807 non-null  float64\n 23  V23     284807 non-null  float64\n 24  V24     284807 non-null  float64\n 25  V25     284807 non-null  float64\n 26  V26     284807 non-null  float64\n 27  V27     284807 non-null  float64\n 28  V28     284807 non-null  float64\n 29  Amount  284807 non-null  float64\n 30  Class   284807 non-null  int64  \ndtypes: float64(30), int64(1)\nmemory usage: 67.4 MB\nNone\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.columns)","execution_count":11,"outputs":[{"output_type":"stream","text":"Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n       'Class'],\n      dtype='object')\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalise the amount column \ndata['normAmount'] = StandardScaler().fit_transform(np.array(data['Amount']).reshape(-1, 1)) \n\n# drop Time and Amount columns as they are not relevant for prediction purpose \ndata = data.drop(['Time', 'Amount'], axis = 1) \n\n# as you can see there are 492 fraud transactions. \ndata['Class'].value_counts() ","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"0    284315\n1       492\nName: Class, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n       'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21',\n       'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Class', 'normAmount'],\n      dtype='object')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data[['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11',\n       'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21',\n       'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28','normAmount']]\ny = data['Class']","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split \n\n# split into 70:30 ration \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0) \n\n# describes info about train and test set \nprint(\"Number transactions X_train dataset: \", X_train.shape) \nprint(\"Number transactions y_train dataset: \", y_train.shape) \nprint(\"Number transactions X_test dataset: \", X_test.shape) \nprint(\"Number transactions y_test dataset: \", y_test.shape) ","execution_count":18,"outputs":[{"output_type":"stream","text":"Number transactions X_train dataset:  (199364, 29)\nNumber transactions y_train dataset:  (199364,)\nNumber transactions X_test dataset:  (85443, 29)\nNumber transactions y_test dataset:  (85443,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Train model without handling imbalanced class distribution\n\n# logistic regression object \nlr = LogisticRegression() \n  \n# train the model on train set \nlr.fit(X_train, y_train.ravel()) \n  \npredictions = lr.predict(X_test) \n  \n# print classification report \nprint(classification_report(y_test, predictions)) \n","execution_count":19,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00     85296\n           1       0.88      0.62      0.73       147\n\n    accuracy                           1.00     85443\n   macro avg       0.94      0.81      0.86     85443\nweighted avg       1.00      1.00      1.00     85443\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"accuracy of 100% but the minority class has 62% recall"},{"metadata":{},"cell_type":"markdown","source":"**SMOTE**"},{"metadata":{},"cell_type":"markdown","source":"Oversampling of the minority with synthetic data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1))) \nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0))) \n  \n# import SMOTE module from imblearn library \n# pip install imblearn (if you don't have imblearn in your system) \nfrom imblearn.over_sampling import SMOTE \nsm = SMOTE(random_state = 2) #k_neighbors (default=5)\nX_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel()) \n  \nprint('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape)) \nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape)) \n  \nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res == 1))) \nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res == 0))) \n","execution_count":20,"outputs":[{"output_type":"stream","text":"Before OverSampling, counts of label '1': 345\nBefore OverSampling, counts of label '0': 199019 \n\nAfter OverSampling, the shape of train_X: (398038, 29)\nAfter OverSampling, the shape of train_y: (398038,) \n\nAfter OverSampling, counts of label '1': 199019\nAfter OverSampling, counts of label '0': 199019\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr1 = LogisticRegression() \nlr1.fit(X_train_res, y_train_res.ravel()) \npredictions = lr1.predict(X_test) \n  \n# print classification report \nprint(classification_report(y_test, predictions)) \n","execution_count":21,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       1.00      0.98      0.99     85296\n           1       0.06      0.92      0.11       147\n\n    accuracy                           0.98     85443\n   macro avg       0.53      0.95      0.55     85443\nweighted avg       1.00      0.98      0.99     85443\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Recall increased to 92%"},{"metadata":{},"cell_type":"markdown","source":"**NearMiss Algorithm**\n\nUndersampled the majority instances and made it equal to majority class"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nprint(\"Before Undersampling, counts of label '1': {}\".format(sum(y_train == 1))) \nprint(\"Before Undersampling, counts of label '0': {} \\n\".format(sum(y_train == 0))) \n  \n# apply near miss \nfrom imblearn.under_sampling import NearMiss \nnr = NearMiss() \n  \nX_train_miss, y_train_miss = nr.fit_sample(X_train, y_train.ravel()) \n  \nprint('After Undersampling, the shape of train_X: {}'.format(X_train_miss.shape)) \nprint('After Undersampling, the shape of train_y: {} \\n'.format(y_train_miss.shape)) \n  \nprint(\"After Undersampling, counts of label '1': {}\".format(sum(y_train_miss == 1))) \nprint(\"After Undersampling, counts of label '0': {}\".format(sum(y_train_miss == 0))) \n","execution_count":22,"outputs":[{"output_type":"stream","text":"Before Undersampling, counts of label '1': 345\nBefore Undersampling, counts of label '0': 199019 \n\nAfter Undersampling, the shape of train_X: (690, 29)\nAfter Undersampling, the shape of train_y: (690,) \n\nAfter Undersampling, counts of label '1': 345\nAfter Undersampling, counts of label '0': 345\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# train the model on train set \nlr2 = LogisticRegression() \nlr2.fit(X_train_miss, y_train_miss.ravel()) \npredictions = lr2.predict(X_test) \n  \n# print classification report \nprint(classification_report(y_test, predictions)) \n","execution_count":23,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       1.00      0.55      0.71     85296\n           1       0.00      0.95      0.01       147\n\n    accuracy                           0.56     85443\n   macro avg       0.50      0.75      0.36     85443\nweighted avg       1.00      0.56      0.71     85443\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"The recall of the postiive class is 95% (slightly less than SMOTE) but the recall of the majority class has decreased significantly. Therefore SMOTE is better in this situation."}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}